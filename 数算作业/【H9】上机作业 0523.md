# 【H9】上机作业 0523

萧仰哲 2000011478 20210521

---

## 一、用递归算法实现二分搜索，避免用切片操作。通过一个随机产生的有序表（百万级别）来与课件中采用切片操作的二分搜索比较性能。

将`slicing`换成`indexing`：在课件上原有的代码基础上，将`[:mid]` 以及 `[mid+1:]` 改成 `[0:mid]`以及`[mid+1:-1]`即可

```python
binarySearch(lst[0:mid],item)
binarySearch(lst[mid:-1],item)
```

实验方法：进行 $n$ 次 `binarySearch_Indexing` 和 `binarySearch_slicing`，比较花费的时间，记录每种算法耗时较短的次数。

```python
print('Average time of indexing: ', sum(interval_indexing_lst)/n)
print('Average time of slicing: ', sum(interval_slicing_lst)/n)
print('In %d tests, slicing is faster in %d of the cases, while indexing is faster in %d of the cases'%\
    (n,slicing_is_faster,indexing_is_faster))
i_n = (indexing_is_faster/n) * 100
s_n = (slicing_is_faster/n) * 100
print('Slicing is faster takes %f ' % (s_n))
print('Indexing is faster takes %f ' % (i_n))
```

当 $n = 1000$ 时：

```python
Average time of indexing:  0.004530051708221436
Average time of slicing:  0.0043506348133087154
In 1000 tests, slicing is faster in 659 of the cases, while indexing is faster in 341 of the cases
Slicing is faster takes 65.900000
Indexing is faster takes 34.100000
```

由此知`slicing`的性能还是比`indexing`好$^1$。

$^1$ 测试者进行了多次试验，发现多数时候`slicing`比`indexing`好，但偶尔也会有`indexing`比`slicing`好的时候；然而当$n$ 越大的时候，`slicing`与`indexing`胜出的比例也会越悬殊（如当 $n=10000$ 时，`slicing_win : indexing_win = 4:1`）

## 二、采用数据链（chaining）的冲突解决技术来实现ADT Map，要求包括ADT Map中定义的所有方法。

1. 大部分代码与课件相同，这里只阐述与课件不同的地方
2. 首先建立`class Node`，使之具有储存 `key, data`的功能。后面使用`list`进行`Node`的串联，因此这里不需要设定指针 `self.next`

    ```python
    class Node:
        def __init__(self,key,data):
            self.key = key
            self.data = data
        def __del__(self):
            return
    ```

3. 在`class HashTable`增加了一个长度变数，用来储存长度，透过在 `put` 和 `del` 的操作次数来控制长度

    ```python
    class HashTable():
        def __init__(self):
            self.size = 11
            self.slots = [None] * self.size
            self.data = [[None]] * self.size
            self.length = 0
    ```

    实现删除`key-data pair`：因为`node`里面也有储存`key`的信息，在透过`hashfunction`计算出位置后，可以拿`key`与每一个`node`里面的`key`所比对，如果找到相同的，就把它删掉，这个的原理与后面的`get`相同

    ```python
    def __delitem__(self,key): # del map[key]
            # 先把key映射到slot的pos上，然后再根据pos把key和data都删掉
            self.length -= 1
            pos = self.slots.index(key)
            del self.slots[pos]

            hashvalue = self.hashfunction(key)
            for node in self.data[hashvalue]:
                if node.key == key:
                    del node
    def __setitem__(self,key,val):
            self.put(key,val)
            self.length += 1
    ```

4. 实现chaining，在建立`self.data`的时候，采用二维数组，第一维与`self.slots` 对应，第二维储存同一个`key`中的所有`data`的`node`，实现方法就是单纯的`append`在后面。

    ```python
    def put(self,key,data):
            hashvalue = self.hashfunction(key)

            if self.slots[hashvalue] == None: 
                self.slots[hashvalue] = key
                self.data[hashvalue][0] = Node(key,data)
            else:
                if self.slots[hashvalue] == key:
                    self.data[hashvalue][0] = Node(key,data) # replace
                else:
                    self.data[hashvalue].append(Node(key,data))
    ```

5. 实现key in map，利用魔术方法`__contains__`，容易检查`key`有没有在`self.slots`中

    ```python
    def __contains__(self,key): # key in map
            if key in self.slots:
                return True
            else:
                return False
    ```

6. 测试代码

    ```python
    test = HashTable()
    test[1] = 'test1'
    print('len of test is: ',len(test))
    print(1 in test)
    del test[1]
    print(1 in test)
    print('len of test is: ',len(test))
    ```

7. 测试结果

    ```python
    '''
    len of test is:  1
    True
    False
    len of test is:  0
    '''
    ```

## 三、采用开放定址冲突解决技术的散列表，课件中是固定大小的，如果希望在负载因子达到某个阈值之后，散列表的大小能自动增长，该如何设计算法？请写一个算法说明，并实现之。

> $\text{散列表的负载因子} =\frac{\text{填入表中的元素个数}}{\text{散列表的长度}}$

算法说明：一旦负载因子超过 $k$，增加散列表的长度就增加 $n$。

以下 $k =0.7, n =10$

```python
def put(self,key,data):
        hashvalue = self.hashfunction(key)

        if self.slots[hashvalue] == None: 
            self.slots[hashvalue] = key
            self.data[hashvalue][0] = Node(key,data)
        else:
            if self.slots[hashvalue] == key:
                self.data[hashvalue][0] = Node(key,data) # replace
            else:
                self.data[hashvalue].append(Node(key,data))
        
        if self.loadfactor > 0.7:
            self.size += 10
            self.slots.extend([None]*10)
            self.data.extend([[None]]*10)
```

## 四、自行设计一种取“中值”的方法实现快速排序，并与课件中的快速排序比较性能。请写一个算法说明和分析。

以列表的最后一个数字为中值进行快速排序

```python
def partition_new(lst,first,last):
    i = (first-1)
    pivot = lst[last]
  
    for j in range(first, last):
        if lst[j] <= pivot:
            i = i+1
            lst[i], lst[j] = lst[j], lst[i]
    lst[i+1], lst[last] = lst[last], lst[i+1]
    return (i+1)
```

创建`test case` ：

```python
for i in range(100):
    lst.append(random.randint(1,100000))
os.system('echo %s | pbcopy' % (str(lst)))
```

长`10000`的数组，试比较排序速度

```python
lst_1 = [79898, 21436, 1159, 91892, 31147, 71841, 33975, 86340, 30282, 54989, 67724, 59841, 55843, 53898, 39310, 25448, 29463, 33028, 61860, 39610, 8899, 36584, 37332, 69990, 41524, 77020, 21484, 99555, 69367, 67619, 82934, 26942, 68229, 93507, 14772, 50095, 7266, 85648, 25491, 79858, 8110, 81084, 69258, 5656, 85657, 95283, 48192, 77500, 72724, 61648, 26218, 90861, 23303, 34019, 63625, 77844, 84584, 78063, 31655, 28457, 80915, 13301, 52805, 58221, 15793, 80158, 63011, 45403, 42245, 8485, 26984, 35340, 48630, 95351, 38018, 50930, 52638, 87525, 72559, 5951, 8862, 75599, 36996, 30685, 35353, 49508, 84357, 95731, 26220, 79499, 15746, 83810, 41915, 88654, 78586, 96164, 85108, 63082, 27690, 40072]
lst_2 = [79898, 21436, 1159, 91892, 31147, 71841, 33975, 86340, 30282, 54989, 67724, 59841, 55843, 53898, 39310, 25448, 29463, 33028, 61860, 39610, 8899, 36584, 37332, 69990, 41524, 77020, 21484, 99555, 69367, 67619, 82934, 26942, 68229, 93507, 14772, 50095, 7266, 85648, 25491, 79858, 8110, 81084, 69258, 5656, 85657, 95283, 48192, 77500, 72724, 61648, 26218, 90861, 23303, 34019, 63625, 77844, 84584, 78063, 31655, 28457, 80915, 13301, 52805, 58221, 15793, 80158, 63011, 45403, 42245, 8485, 26984, 35340, 48630, 95351, 38018, 50930, 52638, 87525, 72559, 5951, 8862, 75599, 36996, 30685, 35353, 49508, 84357, 95731, 26220, 79499, 15746, 83810, 41915, 88654, 78586, 96164, 85108, 63082, 27690, 40072]


print(len(lst_1))
start1 = time.time()
qsort_new(lst_1)
end1 = time.time()
interval1 = end1 - start1
start2 = time.time()
qsort_old(lst_2)
end2 = time.time()
interval2 = end2 - start2

print(interval1)
print(interval2)
```

测试结果如下：

```python
100
0.00032901763916015625
0.0003559589385986328
```

知`qsort_new`的performance更好，当然performance与中值的选取关系很大，若选取的中值与实际中值相差距离越短，则耗时越少，因此测试数据的组成对快速排序的表现存在影响。

若无法预先知道数组组成的分布（或给定大量的随机数组），则`qsort_old` 与`qsort_new`的平均performance应该是相等的。

